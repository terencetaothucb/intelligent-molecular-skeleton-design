{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec9fecc-9bc7-4a98-b9f0-b3921f6d034f",
   "metadata": {},
   "source": [
    "导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3827cb-b3f4-48fb-baa7-dd22d558cd3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import shap\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,VotingRegressor \n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as XGB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from joblib import Parallel, delayed\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a7764-cc79-4ea1-979d-9cfdfda0d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('')\n",
    "X = data.iloc[:,1:-2]\n",
    "Y = data.iloc[:,-2]\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85adadb3-333b-47a6-8a93-2acd368f97c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_Valid = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "#Define the hyperparameter grid for each model.\n",
    "parameter_XGBR = {\n",
    "}\n",
    "\n",
    "parameter_RF = {\n",
    "}\n",
    "\n",
    "parameter_CBR = {\n",
    "}\n",
    "\n",
    "parameter_LGBM = {\n",
    "}\n",
    "\n",
    "parameter_ABR = {\n",
    "}\n",
    "\n",
    "parameter_GBRT = {\n",
    "}\n",
    "\n",
    "# Define model dictionary\n",
    "estimators = {\n",
    "    'XGBR': XGB.XGBRegressor(random_state=0),\n",
    "    'RF': RandomForestRegressor(random_state=0),\n",
    "    'CBR': CatBoostRegressor(verbose=False, random_state=0),\n",
    "    'LGBM': LGBMRegressor(random_state=0, verbosity=-1),\n",
    "    'ABR': AdaBoostRegressor(random_state=0),\n",
    "    'GBRT': GradientBoostingRegressor(random_state=0)\n",
    "}\n",
    "\n",
    "# Map model names to corresponding hyperparameters.\n",
    "params_mapping = {\n",
    "    'XGBR': parameter_XGBR,\n",
    "    'RF': parameter_RF,\n",
    "    'CBR': parameter_CBR,\n",
    "    'LGBM': parameter_LGBM,\n",
    "    'ABR': parameter_ABR,\n",
    "    'GBRT': parameter_GBRT\n",
    "}\n",
    "\n",
    "grid_searches = {}\n",
    "for name, estimator in estimators.items():\n",
    "    params = params_mapping[name]\n",
    "    grid_searches[name] = GridSearchCV(estimator, params, scoring='r2', cv=cross_Valid, n_jobs=-1)\n",
    "\n",
    "# Train the model and find the best parameters.\n",
    "for name, grid in grid_searches.items():\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(f\"{name} best parameters: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c634a1-52a1-453d-8015-5a6b1c326710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to initialize the model dictionary.\n",
    "def initialize_best_estimators(grid_searches):\n",
    "    return {\n",
    "        'XGBR': XGB.XGBRegressor(**grid_searches['XGBR'].best_params_, random_state=42),\n",
    "        'RF': RandomForestRegressor(**grid_searches['RF'].best_params_, random_state=42),\n",
    "        'CBR': CatBoostRegressor(**grid_searches['CBR'].best_params_, verbose=False, random_state=42),\n",
    "        'LGBM': LGBMRegressor(**grid_searches['LGBM'].best_params_, random_state=42, verbosity=-1),\n",
    "        'ABR': AdaBoostRegressor(**grid_searches['ABR'].best_params_, random_state=42),\n",
    "        'GBRT': GradientBoostingRegressor(**grid_searches['GBRT'].best_params_, random_state=42)\n",
    "    }\n",
    "\n",
    "# Define a function to calculate model performance and feature importance.\n",
    "def evaluate_models(seed, X, Y, grid_searches, submodel_r2_sums, submodel_rmse_sums, num_seeds):\n",
    "    cross_validator = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    best_estimators = initialize_best_estimators(grid_searches)\n",
    "\n",
    "   # Create a VotingRegressor\n",
    "    submodels = [(name, estimator) for name, estimator in best_estimators.items() if estimator is not None]\n",
    "    voting_regressor = VotingRegressor(submodels)\n",
    "\n",
    "    # Calculate the mean values of R2 and RMSE for each submodel.\n",
    "    submodel_r2_means = {}\n",
    "    submodel_rmse_means = {}\n",
    "    rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
    "    \n",
    "    # Used to store the feature importance of each model.\n",
    "    submodel_feature_importances = {}\n",
    "    feature_importances_weighted_sum = np.zeros(X.shape[1])\n",
    "    total_r2 = 0\n",
    "\n",
    "    for name, estimator in submodels:\n",
    "        r2_scores = cross_val_score(estimator, X, Y, cv=cross_validator, scoring='r2', n_jobs=-1)\n",
    "        rmse_scores = cross_val_score(estimator, X, Y, cv=cross_validator, scoring=rmse_scorer, n_jobs=-1)\n",
    "        submodel_r2_means[name] = np.mean(r2_scores)\n",
    "        submodel_rmse_means[name] = np.mean(rmse_scores)\n",
    "        submodel_r2_sums[name] += submodel_r2_means[name]\n",
    "        submodel_rmse_sums[name] += submodel_rmse_means[name]\n",
    "\n",
    "        # Calculate feature importance\n",
    "        estimator.fit(X, Y)\n",
    "        importances = estimator.feature_importances_\n",
    "        \n",
    "        # Normalization processing\n",
    "        importances_normalized = importances / np.sum(importances)\n",
    "        \n",
    "        submodel_feature_importances[name] = dict(zip(X.columns, importances_normalized))\n",
    "        feature_importances_weighted_sum += importances_normalized * submodel_r2_means[name]\n",
    "        total_r2 += submodel_r2_means[name]\n",
    "\n",
    "    # Calculate the mean R2 and weighted RMSE of the fusion model\n",
    "    voting_regressor_r2_mean = np.mean(cross_val_score(voting_regressor, X, Y, cv=cross_validator, scoring='r2', n_jobs=-1))\n",
    "    voting_regressor_rmse_mean = np.mean(cross_val_score(voting_regressor, X, Y, cv=cross_validator, scoring=rmse_scorer, n_jobs=-1))\n",
    "\n",
    "    # Calculate weighted feature importance\n",
    "    weighted_feature_importances = feature_importances_weighted_sum / total_r2 if total_r2 != 0 else feature_importances_weighted_sum\n",
    "\n",
    "    weighted_feature_importances_dict = dict(zip(X.columns, weighted_feature_importances))\n",
    "\n",
    "    return {\n",
    "        'submodel_r2_means': submodel_r2_means,\n",
    "        'submodel_rmse_means': submodel_rmse_means,\n",
    "        'voting_regressor_r2_mean': voting_regressor_r2_mean,\n",
    "        'voting_regressor_rmse_mean': voting_regressor_rmse_mean,\n",
    "        'submodel_feature_importances': submodel_feature_importances,\n",
    "        'weighted_feature_importances': weighted_feature_importances_dict\n",
    "    }\n",
    "\n",
    "# Define to save all seed results.\n",
    "all_results = []\n",
    "seeds = range(100)\n",
    "\n",
    "# Initialize the cumulative sum of R2 and RMSE of submodels.\n",
    "submodel_r2_sums = {name: 0.0 for name in initialize_best_estimators(grid_searches).keys()}\n",
    "submodel_rmse_sums = {name: 0.0 for name in initialize_best_estimators(grid_searches).keys()}\n",
    "\n",
    "# Initialize the cumulative sum of weighted feature importances of the ensemble model.\n",
    "weighted_feature_importances_sums = np.zeros(len(X.columns))\n",
    "\n",
    "# Traverse seeds and save results\n",
    "for seed in seeds:\n",
    "    result = evaluate_models(seed, X, Y, grid_searches, submodel_r2_sums, submodel_rmse_sums, len(seeds))\n",
    "    all_results.append(result)\n",
    "    weighted_feature_importances_sums += np.array(list(result['weighted_feature_importances'].values()))\n",
    "    print(f\"\")\n",
    "    print(f\"Seed {seed} - Voting Regressor R2 Mean: {result['voting_regressor_r2_mean']}, RMSE Mean: {result['voting_regressor_rmse_mean']}\")\n",
    "    print(\"Submodel R2 and RMSE Means:\")\n",
    "    for name in result['submodel_r2_means'].keys():\n",
    "        print(f\"{name}: R2 Mean = {result['submodel_r2_means'][name]}, RMSE Mean = {result['submodel_rmse_means'][name]}\")\n",
    "\n",
    "# Calculate the average value of the weighted feature importance of the fusion model for all seeds.\n",
    "avg_weighted_feature_importances = weighted_feature_importances_sums / len(seeds)\n",
    "\n",
    "# Bind the feature names and the corresponding weighted feature importances and sort them in descending order of importance.\n",
    "sorted_feature_importances = sorted(zip(X.columns, avg_weighted_feature_importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "Output the average R2, RMSE of the fusion model and the sorted feature importance.\n",
    "avg_voting_regressor_r2_mean = np.mean([result['voting_regressor_r2_mean'] for result in all_results])\n",
    "avg_voting_regressor_rmse_mean = np.mean([result['voting_regressor_rmse_mean'] for result in all_results])\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"Average Voting Regressor R2 Mean: {avg_voting_regressor_r2_mean}\")\n",
    "print(f\"Average Voting Regressor Weighted RMSE Mean: {avg_voting_regressor_rmse_mean}\")\n",
    "print(\"Average Submodel R2 and RMSE Means:\")\n",
    "for name in submodel_r2_sums.keys():\n",
    "    print(f\"{name}: R2 Mean = {submodel_r2_sums[name] / len(seeds)}, RMSE Mean = {submodel_rmse_sums[name] / len(seeds)}\")\n",
    "    \n",
    "# Draw a bar chart of feature importances\n",
    "features, importances = zip(*sorted_feature_importances)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=list(importances), y=list(features))\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances Sorted by Importance')\n",
    "plt.show()\n",
    "\n",
    "print(\"Sorted Feature Importances:\")\n",
    "for feature, importance in sorted_feature_importances:\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
